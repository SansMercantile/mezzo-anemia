# backend/mezzo_anima_line/memory_ingestion.py

import logging
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from datetime import datetime
import uuid
import json # Added for parsing LLM responses

# Assuming LLMClient is available from backend.ai_ops
# We'll pass an instance of LLMClient during initialization.
from mpeti_modules.ai_ops import LLMClient 

logger = logging.getLogger(__name__)

class Memory(BaseModel):
    """Represents a single piece of ingested memory (e.g., a letter, message, journal entry)."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    source_type: str  # e.g., 'email', 'journal', 'sms', 'document'
    content: str
    timestamp: datetime
    author: str
    extracted_entities: List[str] = Field(default_factory=list) # People, places, things mentioned
    sentiment_score: float = 0.0 # Positive/negative sentiment from -1.0 to 1.0
    summary: Optional[str] = None # A brief summary generated by LLM

class MemoryIngestionPipeline:
    """
    A pipeline for processing raw text data into structured Memory objects
    using an LLM for advanced NLP tasks like entity extraction and sentiment analysis.
    """
    def __init__(self, llm_client: LLMClient):
        logger.info("MemoryIngestionPipeline initialized with LLMClient.")
        self.llm_client = llm_client
        self.is_llm_ready = False # Flag to track LLM readiness

    async def initialize_llm_pipeline(self):
        """Checks if the underlying LLM client is ready."""
        if self.llm_client:
            self.is_llm_ready = await self.llm_client.wait_for_initialization()
            if not self.is_llm_ready:
                logger.warning("MemoryIngestionPipeline: LLMClient not ready. NLP features will be limited or unavailable.")
        else:
            logger.warning("MemoryIngestionPipeline: LLMClient not provided. NLP features will be unavailable.")

    async def process_text(self, raw_text: str, source_type: str, author: str, timestamp: datetime) -> Memory:
        """
        Processes a raw string of text into a structured Memory object using LLM for NLP.
        """
        logger.info(f"Processing new memory from source: {source_type} by {author} at {timestamp}")
        
        extracted_entities: List[str] = []
        sentiment_score: float = 0.0
        summary: Optional[str] = None

        if self.is_llm_ready:
            try:
                # --- LLM for Entity Extraction, Sentiment, and Summary ---
                prompt = f"""Analyze the following text and provide:
                1. A list of key entities (people, organizations, locations).
                2. A sentiment score between -1.0 (very negative) and 1.0 (very positive).
                3. A concise summary (max 3 sentences).

                Respond in JSON format with keys: "entities", "sentiment_score", "summary".

                Text: \"\"\"{raw_text}\"\"\"
                """
                
                # Use a structured response schema for the LLM call
                response_schema = {
                    "type": "OBJECT",
                    "properties": {
                        "entities": {
                            "type": "ARRAY",
                            "items": {"type": "STRING"}
                        },
                        "sentiment_score": {"type": "NUMBER"},
                        "summary": {"type": "STRING"}
                    },
                    "required": ["entities", "sentiment_score", "summary"]
                }

                llm_response_json_str = await self.llm_client.generate_structured_content(
                    prompt=prompt,
                    response_schema=response_schema,
                    temperature=0.2 # Lower temperature for more factual extraction
                )

                if llm_response_json_str:
                    try:
                        llm_output = json.loads(llm_response_json_str)
                        extracted_entities = llm_output.get("entities", [])
                        sentiment_score = float(llm_output.get("sentiment_score", 0.0))
                        summary = llm_output.get("summary")
                        logger.info(f"LLM processed memory: Entities={extracted_entities}, Sentiment={sentiment_score}, Summary={summary}")
                    except json.JSONDecodeError as jde:
                        logger.error(f"Failed to parse LLM JSON response for memory ingestion: {jde}. Raw response: {llm_response_json_str[:200]}", exc_info=True)
                    except ValueError as ve:
                        logger.error(f"Invalid value in LLM JSON response for memory ingestion: {ve}. Raw response: {llm_response_json_str[:200]}", exc_info=True)
                else:
                    logger.warning("LLM returned no content for memory ingestion. Using default values.")

            except Exception as e:
                logger.error(f"Error during LLM processing of memory: {e}", exc_info=True)
                # Fallback to basic processing if LLM fails
        
        # Create the Memory object
        memory = Memory(
            source_type=source_type,
            content=raw_text,
            timestamp=timestamp,
            author=author,
            extracted_entities=extracted_entities,
            sentiment_score=sentiment_score,
            summary=summary
        )
        return memory

# Note: The MemoryIngestionPipeline should be instantiated and initialized
# in mpeti_core.py, passing the LLMClient instance.
